{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4wYNVJYLc3zdUb8RB7eax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imemmul/google-research/blob/rarity/dpok/dpok_nft/ImageReward/score_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UFY7VNryAEh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/imemmul/google-research.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V7DsgRaCyF11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "mjr5Nbf0yIJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd google-research/dpok/dpok_nft"
      ],
      "metadata": {
        "id": "BXuYBeAxyKww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_clip.py\n",
        "import os\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from ImageReward.models.CLIPScore import CLIPScore\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Run aesthetic scoring on a dataset using CLIP.\")\n",
        "    parser.add_argument(\"--dataset_path\", type=str, required=True, help=\"Path to the dataset containing images.\")\n",
        "    # parser.add_argument(\"--prompt\", type=str, default=\"aesthetic beauty\", help=\"Text prompt describing the aesthetic criteria.\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    model_name = \"openai/clip-vit-large-patch14\"\n",
        "    clip_score = CLIPScore(model_name, device=\"cuda\")\n",
        "    #transform = transforms.Compose([\n",
        "    #    transforms.Resize((224, 224)),\n",
        "    #    transforms.ToTensor(),\n",
        "    #    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    #])\n",
        "\n",
        "    image_paths = [os.path.join(args.dataset_path, filename) for filename in os.listdir(args.dataset_path) if filename.endswith((\".png\"))]\n",
        "    scores = []\n",
        "    indexes = np.load(\"/content/drive/MyDrive/FID_evaluation/indexes.npy\")\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/output_captioning/NFT_DATASET_MERGED/train/metadata.csv\")\n",
        "    prompts = [df.iloc[idx]['text'] for idx in indexes]\n",
        "    for prompt, image_path in zip(prompts, image_paths):\n",
        "        #image = transform(image).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "        score = clip_score.score(prompt, image_path)\n",
        "        scores.append(score)\n",
        "\n",
        "    final_clip_score = sum(scores) / len(scores)\n",
        "    print(\"Final CLIP Score:\", final_clip_score)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_Ov5EslXyN-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "vJWbx062ySD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "OnhYC1DByVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairscale"
      ],
      "metadata": {
        "id": "z57dhCYcyYfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clip.py --dataset_path /content/drive/MyDrive/FID_evaluation/real_images"
      ],
      "metadata": {
        "id": "-dPTJ6VgybQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clip.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_sft"
      ],
      "metadata": {
        "id": "K0w4vc5Kyh65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clip.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_dpok_sonolsunartik"
      ],
      "metadata": {
        "id": "2rrDsp7Uykgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile med_config.json\n",
        "{\n",
        "  \"architectures\": [\n",
        "    \"BertModel\"\n",
        "  ],\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"layer_norm_eps\": 1e-12,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"model_type\": \"bert\",\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 30524,\n",
        "  \"encoder_width\": 768,\n",
        "  \"add_cross_attention\": true\n",
        "}"
      ],
      "metadata": {
        "id": "8_ZaxHKgynM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_blip.py\n",
        "import os\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from ImageReward.models.BLIPScore import BLIPScore\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Run aesthetic scoring on a dataset using BLIP.\")\n",
        "    parser.add_argument(\"--dataset_path\", type=str, required=True, help=\"Path to the dataset containing images.\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    blip_score = BLIPScore(\"/content/google-research/dpok/dpok_nft/med_config.json\", device=\"cpu\")\n",
        "\n",
        "    image_paths = [os.path.join(args.dataset_path, filename) for filename in os.listdir(args.dataset_path) if filename.endswith((\".png\"))]\n",
        "    scores = []\n",
        "    indexes = np.load(\"/content/drive/MyDrive/FID_evaluation/indexes.npy\")\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/output_captioning/NFT_DATASET_MERGED/train/metadata.csv\")\n",
        "    prompts = [df.iloc[idx]['text'] for idx in indexes]\n",
        "\n",
        "    for prompt, image_path in zip(prompts, image_paths):\n",
        "        score = blip_score.score(prompt, image_path)\n",
        "        scores.append(score)\n",
        "\n",
        "    final_blip_score = sum(scores) / len(scores)\n",
        "    print(\"Final BLIP Score:\", final_blip_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ksyzpCnRyqSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_blip.py --dataset_path /content/drive/MyDrive/FID_evaluation/real_images"
      ],
      "metadata": {
        "id": "k46niIGryvG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_blip.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_sft"
      ],
      "metadata": {
        "id": "li1I1WQ4y0MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_blip.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_dpok_sonolsunartik"
      ],
      "metadata": {
        "id": "YKdRFWdry5aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_ir.py\n",
        "import os\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from ImageReward.ImageReward import ImageReward\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Run aesthetic scoring on a dataset using ImageReward.\")\n",
        "    parser.add_argument(\"--dataset_path\", type=str, required=True, help=\"Path to the dataset containing images.\")\n",
        "    parser.add_argument(\"--med_config\", type=str, required=True, help=\"Path to the med_config.json file.\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    image_reward = ImageReward(args.med_config, device=\"cpu\")\n",
        "\n",
        "    image_paths = [os.path.join(args.dataset_path, filename) for filename in os.listdir(args.dataset_path) if filename.endswith((\".png\"))]\n",
        "    scores = []\n",
        "    indexes = np.load(\"/content/drive/MyDrive/FID_evaluation/indexes.npy\")\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/output_captioning/NFT_DATASET_MERGED/train/metadata.csv\")\n",
        "    prompts = [df.iloc[idx]['text'] for idx in indexes]\n",
        "\n",
        "    for prompt, image_path in zip(prompts, image_paths):\n",
        "        score = image_reward.score(prompt, image_path)\n",
        "        scores.append(score)\n",
        "\n",
        "    final_image_reward_score = sum(scores) / len(scores)\n",
        "    print(\"Final ImageReward Score:\", final_image_reward_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hIe-S2gqy9Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ir.py --dataset_path /content/drive/MyDrive/FID_evaluation/real_images --med_config /content/google-research/dpok/dpok_nft/med_config.json"
      ],
      "metadata": {
        "id": "HZJQYINqy94T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ir.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_sft --med_config /content/google-research/dpok/dpok_nft/med_config.json"
      ],
      "metadata": {
        "id": "1sjmfjDuzB63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ir.py --dataset_path /content/drive/MyDrive/FID_evaluation/generated_dpok_sonolsunartik --med_config /content/google-research/dpok/dpok_nft/med_config.json"
      ],
      "metadata": {
        "id": "7sftuCXVzKfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_as.py\n",
        "import os\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from ImageReward.models.AestheticScore import AestheticScore\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Run aesthetic scoring on a dataset using AestheticScore.\")\n",
        "    parser.add_argument(\"--dataset_path\", type=str, required=True, help=\"Path to the dataset containing images.\")\n",
        "    #parser.add_argument(\"--download_root\", type=str, required=True, help=\"Path to the directory where the CLIP model is downloaded.\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    aesthetic_score = AestheticScore(\"openai/clip-vit-large-patch14\", device=\"cpu\")\n",
        "    image_paths = [os.path.join(args.dataset_path, filename) for filename in os.listdir(args.dataset_path) if filename.endswith((\".png\"))]\n",
        "    scores = []\n",
        "    indexes = np.load(\"/content/drive/MyDrive/FID_evaluation/indexes.npy\")\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/output_captioning/NFT_DATASET_MERGED/train/metadata.csv\")\n",
        "    prompts = [df.iloc[idx]['text'] for idx in indexes]\n",
        "\n",
        "    for prompt, image_path in zip(prompts, image_paths):\n",
        "        score = aesthetic_score.score(prompt, image_path)\n",
        "        scores.append(score)\n",
        "\n",
        "    final_aesthetic_score = sum(scores) / len(scores)\n",
        "    print(\"Final Aesthetic Score:\", final_aesthetic_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "dT0dGrntzNtB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}